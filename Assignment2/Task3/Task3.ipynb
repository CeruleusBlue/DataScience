{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "    <h5>The cell below: </h5>\n",
    "    <ul>\n",
    "        <li>Reads the data from the file onto a spark dataframes</li>\n",
    "        <br>\n",
    "        <li>Separates the uris and hyperlinks(dropping the rdf prefixes) into two dataframes</li>\n",
    "        <br>\n",
    "        <li>Creates views for the dataframes so that sql queries can be performed on them</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/28 14:59:37 WARN Utils: Your hostname, MSI resolves to a loopback address: 127.0.1.1; using 172.23.129.232 instead (on interface eth0)\n",
      "22/10/28 14:59:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/28 14:59:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark, pyspark.sql \n",
    "from pyspark.sql.functions import *  \n",
    "import numpy as np\n",
    "\n",
    "spark:pyspark.sql.SparkSession = pyspark.sql.SparkSession.Builder().getOrCreate()\n",
    "\n",
    "df = spark.read.csv('Task2/gr0.California', sep=\" \")\n",
    "uri = df.filter(\"_c0 = 'n'\").drop('_c0')\n",
    "hyperlink = df.filter(\"_c0 = 'e'\").drop('_c0')\n",
    "uri.createGlobalTempView('uri')\n",
    "hyperlink.createGlobalTempView('hyperlink')\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "    <h5>The cell below: </h5>\n",
    "    <ul>\n",
    "        <li>Runs an sql query which returns the out-degree for each distinct value of uri present in the out link column of hyperlink.</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[link: string, count: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outLinkCount = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT _c1 AS link, count(*) AS count\n",
    "    FROM global_temp.hyperlink\n",
    "    GROUP BY _c1\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "    <h5>The cell below: </h5>\n",
    "    <ul>\n",
    "        <li>Stores the dimension of the transition matrix, the dimension of the Block Matrix, and the dimension of each block of the block matrix</li>\n",
    "        <br>\n",
    "        <li>Maps the inlink, the outlink and the transition probability into an rdd for the creation of the transition block matrix</li>\n",
    "        <br>\n",
    "        <li>Creates a coordinate matrix with the aforementioned rdd and x and y dimensions equal to the dimension of the transition matrix which is then converted into a block matrix with dimension of each block passed as x and y dimensions</li>\n",
    "        <br>\n",
    "        <li>Maps the id column of the uri into an rdd with id as row index, 0 as the column index and probability of the id as the matrix entry </li>\n",
    "        <br>\n",
    "        <li>Creates a coordinate matrix with the aforementioned rdd and x dimension equal to the dimension of the transition matrix and y equal to 1 which is then converted into a block matrix with dimension of each block and 1 passed as x and y dimensions respectively</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.linalg.distributed.BlockMatrix at 0x7f0533887f70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import *\n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "\n",
    "#dimension of transition matrix\n",
    "matrixSize = uri.count()\n",
    "#dimension of block matrix\n",
    "totalBlocks = 5\n",
    "#dimension of each block of the block matrix\n",
    "blockSize = int(np.ceil(matrixSize/totalBlocks))\n",
    "\n",
    "transRdd = hyperlink.withColumnRenamed('_c1','link')\\\n",
    "            .join(outLinkCount, on='link', how='full')\\\n",
    "            .rdd.map(lambda x: \n",
    "                [x.__getitem__('_c2'),\n",
    "                x.__getitem__('link'),\n",
    "                1/x.__getitem__('count')])\n",
    "                \n",
    "transMatrix = CoordinateMatrix(transRdd.map(lambda x: MatrixEntry(*x)), matrixSize, matrixSize)\\\n",
    "    .toBlockMatrix(blockSize,blockSize)\n",
    "\n",
    "rankRdd = uri.select('_c1').rdd.map(lambda x: [int(x.__getitem__(0)), 0, 1/matrixSize])\n",
    "\n",
    "rankMatrix = CoordinateMatrix(rankRdd.map(lambda x: MatrixEntry(*x)), matrixSize, 1)\\\n",
    "                .toBlockMatrix(blockSize, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "    <h5>The cell below: </h5>\n",
    "    <span style=\"font-weight:700\">Iterates for a specific number of times. Within each iteration:</span>\n",
    "    <ul>\n",
    "        <li>The transition matrix is multiplied by the rank matrix with dimensions of the resulting matrix equal to that of the rank matrix</li>\n",
    "        <br>\n",
    "        <li>Each entry of the rank block matrix is mapped into an double dimensional array which results in an rdd with two columns. The first contains the matrix row and column location and the second which contains the aforementioned array.</li>\n",
    "        <br>\n",
    "        <li>The data is made resilient against spider traps with the &beta; value </li>\n",
    "        <br>\n",
    "        <li>The sum of all entries within the block matrix is calculated.</li>\n",
    "        <br>\n",
    "        <li>If the sum is zero the loop terminates</li>\n",
    "        <br>\n",
    "        <li>The rdd is converted into a Block Matrix again by mapping the array column of the rdd into a Dense Matrix with x equal to the length of the array and y equal to 1</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/28 14:59:48 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/10/28 14:59:48 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/10/28 14:59:48 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 10\n",
    "BETA = 0.85\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    rankMatrix = transMatrix.multiply(rankMatrix)\n",
    "    rankRddIter = rankMatrix.blocks.map(lambda x: (x[0], x[1].toArray()))\n",
    "    rankRddIter = rankRddIter.map(lambda x: (x[0], x[1]*BETA + (1-BETA)/matrixSize))\n",
    "    tol = rankRddIter.map(lambda x: np.sum(x[1])).reduce(lambda x,y : x+y)\n",
    "    if tol > 0:\n",
    "        rankRddIter = rankRddIter.map(lambda x: (x[0], x[1]/tol))\n",
    "    else:\n",
    "        print(\"rank vector sum is 0\")\n",
    "        break\n",
    "    rankMatrix = BlockMatrix(\n",
    "        rankRddIter.map(lambda x: (x[0], DenseMatrix(len(x[1]),1,x[1]))), \n",
    "        blockSize, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+\n",
      "|i  |value                |\n",
      "+---+---------------------+\n",
      "|0  |0.009137321754279665 |\n",
      "|1  |0.0020291038525524067|\n",
      "|2  |5.097803375233102E-5 |\n",
      "|3  |9.829166697808767E-4 |\n",
      "|4  |4.502481223333973E-5 |\n",
      "|5  |1.3177583864833897E-4|\n",
      "|6  |0.010668038246773488 |\n",
      "|7  |2.077105134086428E-5 |\n",
      "|8  |8.193277232323944E-4 |\n",
      "|9  |0.0012574389224513387|\n",
      "|10 |0.005354946580873862 |\n",
      "|11 |2.2228530893132523E-4|\n",
      "|12 |2.743296629456242E-4 |\n",
      "|13 |2.077105134086428E-5 |\n",
      "|14 |9.503722374305101E-4 |\n",
      "|15 |2.1444314180811653E-4|\n",
      "|16 |1.4416432606211525E-4|\n",
      "|17 |0.014369661631170653 |\n",
      "|18 |6.121578086266045E-5 |\n",
      "|19 |5.121200128126079E-4 |\n",
      "+---+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rankMatrix.toCoordinateMatrix().entries.toDF().select('i','value').show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebcb8d90afcb3acd9612fda88f26c241905a567f769a0381f525a5cdcc5f62ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
